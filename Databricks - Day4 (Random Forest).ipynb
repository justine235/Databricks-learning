{"cells":[{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\n\n\n#df = spark.read.format('parquet').table('training_dataset_numeric')\ndf = spark.read.format('parquet').table('training_dataset_numeric')\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/events\")\n\ndf = spark.read.format('delta').load('/delta/events') \ndf = df.select('bathrooms_na','price','minimum_nights','property_type','beds')\ndf = df.na.drop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20faaf29-dfb7-4e9c-9600-1d9d42921291"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = df.filter(df[\"property_type\"].isin(['Private room in condominium (condo)', 'Entire residential home', 'Entire rental unit']))\n                        \ndisplay(df.\n       groupby('property_type').count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"493458dc-8481-40df-b0ae-fd984309913c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Private room in condominium (condo)",99],["Entire residential home",536],["Entire rental unit",1260]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"property_type","type":"\"string\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>property_type</th><th>count</th></tr></thead><tbody><tr><td>Private room in condominium (condo)</td><td>99</td></tr><tr><td>Entire residential home</td><td>536</td></tr><tr><td>Entire rental unit</td><td>1260</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# without hyperparameters tuning\n\n# Transform our Train & Test in delta format\n(Train, Test) = df.randomSplit([.8,.2], seed=42)\n\n# categorial vs numerical\ncategoricalCols = [field for (field, dataType) in Train.dtypes if dataType == \"string\"]\nindexOutputCols = [x + \"Index\" for x in categoricalCols]\nstringIndexer = StringIndexer(inputCols = categoricalCols, outputCols = indexOutputCols, handleInvalid='skip')\n\nnumericalCols = [field for (field, dataType) in Train.dtypes if ((dataType == 'double') & (field != 'price'))]\n\nassemblerInputs = indexOutputCols + numericalCols\nvecAssembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'features')\n\n\nrf = RandomForestRegressor(labelCol=\"price\", maxBins=40)\nstages = [stringIndexer, vecAssembler, rf]\npipeline = Pipeline(stages = stages)\n\npredDF = pipeline.fit(Train).transform(Test)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7393b10d-0327-486d-85db-6f6ac88c3bee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(rf.explainParams())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e78f9d81-a749-40d2-964a-06bcc33b4d36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">bootstrap: Whether bootstrap samples are used when building trees. (default: True)\ncacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: &#39;auto&#39; (choose automatically for task: If numTrees == 1, set to &#39;all&#39;. If numTrees &gt; 1 (forest), set to &#39;sqrt&#39; for classification and to &#39;onethird&#39; for regression), &#39;all&#39; (use all features), &#39;onethird&#39; (use 1/3 of the features), &#39;sqrt&#39; (use sqrt(number of features)), &#39;log2&#39; (use log2(number of features)), &#39;n&#39; (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = &#39;auto&#39; (default: auto)\nfeaturesCol: features column name. (default: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\nlabelCol: label column name. (default: label, current: price)\nleafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\nmaxBins: Max number of bins for discretizing continuous features.  Must be &gt;=2 and &gt;= number of categories for any categorical feature. (default: 32, current: 40)\nmaxDepth: Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1. (default: 1)\nminWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\nnumTrees: Number of trees to train (&gt;= 1). (default: 20)\npredictionCol: prediction column name. (default: prediction)\nseed: random seed. (default: 2502083311556356884)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">bootstrap: Whether bootstrap samples are used when building trees. (default: True)\ncacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: &#39;auto&#39; (choose automatically for task: If numTrees == 1, set to &#39;all&#39;. If numTrees &gt; 1 (forest), set to &#39;sqrt&#39; for classification and to &#39;onethird&#39; for regression), &#39;all&#39; (use all features), &#39;onethird&#39; (use 1/3 of the features), &#39;sqrt&#39; (use sqrt(number of features)), &#39;log2&#39; (use log2(number of features)), &#39;n&#39; (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = &#39;auto&#39; (default: auto)\nfeaturesCol: features column name. (default: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\nlabelCol: label column name. (default: label, current: price)\nleafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\nmaxBins: Max number of bins for discretizing continuous features.  Must be &gt;=2 and &gt;= number of categories for any categorical feature. (default: 32, current: 40)\nmaxDepth: Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1. (default: 1)\nminWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\nnumTrees: Number of trees to train (&gt;= 1). (default: 20)\npredictionCol: prediction column name. (default: prediction)\nseed: random seed. (default: 2502083311556356884)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(labelCol = 'price', predictionCol = 'prediction')\nrmse = evaluator.evaluate(predDF)\nr2 = evaluator.setMetricName('r2').evaluate(predDF)\n\nprint(rmse)\nprint(r2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa01e8b7-e293-4421-a5db-d51cd9a336df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">184.6839422735982\n0.32698469283833875\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">184.6839422735982\n0.32698469283833875\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# with hyperparameters tuning => directly connected with mlflow in background, allows an interative comparaison\nfrom pyspark.ml.tuning import ParamGridBuilder\n\nparamGrid = (ParamGridBuilder()\n            .addGrid(rf.maxDepth, [2,5])\n            .addGrid(rf.numTrees, [5,10])\n            .build())\n\nparamGrid"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79a6724f-63d1-4b83-abe1-3ae9af0f40b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[17]: [{Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 2,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 5},\n {Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 2,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 10},\n {Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 5,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 5},\n {Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 5,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 10}]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[17]: [{Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 2,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 5},\n {Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 2,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 10},\n {Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 5,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 5},\n {Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 5,\n  Param(parent=&#39;RandomForestRegressor_34d5ed4d355d&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (&gt;= 1).&#39;): 10}]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 3K cross validation\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\nevaluator = RegressionEvaluator(labelCol = 'price', predictionCol = 'prediction')\ncv = CrossValidator(estimator = pipeline, evaluator = evaluator, estimatorParamMaps = paramGrid, numFolds = 3, seed = 42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed567112-1a28-4fc9-8b41-4a921d095075"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["cvModel = cv.fit(Train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f48fd409-dcb3-49c1-9e70-3444a49166f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# problem : manually search space so we can miss the optimum one and does have an expense\n# Hyperopt solves this by doing the hyperparameter based on a range of value, could be run with Serial or Parallel\n# 3 options : \n# Random search\n# TPE (tree of parzen Estimators)\n# Adaptive TPE - freeze parameter 1 after result then search for parameter 2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcffcdab-92ea-4645-9487-28a627946574"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport mlflow\n\ndef objective_function(params):\n  max_depth = params['max_depth']\n  num_trees = params['num_trees']\n  print(max_depth)\n  \n  grid = (ParamGridBuilder()\n            .addGrid(rf.maxDepth, ['max_depth'])\n            .addGrid(rf.numTrees, ['num_trees'])\n            .build())\n  \n  cv = CrossValidator(estimator = pipeline, evaluator = evaluator, estimatorParamMaps = paramGrid, numFolds = 3, seed = 42)\n  cvModel = cv.fit(Train)\n  \n  rmse = cvModel.avgMetrics[0]\n  \n  return ('loss', rmse, \"status\", STATUS_OK)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82a269c6-7ae9-4811-b85a-b64b7445a5dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from hyperopt import hp\n\nsearch_space = {\n  \"max_depth\" : hp.choice('max_depth', np.arange(1, 10,dtype=int)),\n  \"num_trees\" : hp.choice('num_trees', np.arange(10, 20,dtype=int))\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3480360-48fa-44fa-ad58-9fee6148d55d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from hyperopt import fmin, tpe, STATUS_OK, Trials\nimport numpy as np \n\nwith mlflow.start_run():\n  num_evals = 4\n  trials = Trials()\n  \n  best_hyperparam = fmin(fn = objective_function,\n                         space = search_space,\n                         algo = tpe.suggest,\n                         max_evals = num_evals, \n                         trials = trials,\n                         rstate = np.random.RandomState(42)\n                        )\n  \n  best_max_depth = best_hyperparam['max_depth']\n  best_num_trees = best_hyperparam['num_trees']\n  \n  # run RF with the best param\n  rf.setMaxDepth(best_max_depth)\n  rf.setNumTrees(best_num_trees)\n  \n  pipelineModel = pipeline.fit(trainDF)\n  \n  predDF = pipelineModel.transform(Test)\n  rmse = regressionEvaluator.evaluate(predDF)\n  \n  mlflow.log_param('max_depth', best_max_depth)\n  mlflow.log_param('num_trees', best_max_depth)\n  mlflow.log_metric('rmse', rmse)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"801375ec-fe1d-45a0-b9f1-64c20076c0ce"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Databricks - Day4 (Random Forest)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1373087798414585}},"nbformat":4,"nbformat_minor":0}
